{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning Projects\n",
    "\n",
    "## Introduction to ML strategy\n",
    "\n",
    "### Orthogonalization\n",
    "\n",
    "- fit training set well on cost function (bigger network, better optimization algorithm)\n",
    "- then, fit dev set well on cost function (regularization, bigger training set)\n",
    "- then, fit test set well on cost function (bigger dev set)\n",
    "- then, perform well in real world (change dev set or cost function)\n",
    "\n",
    "## Setting up your goal\n",
    "\n",
    "### Sinle number evaluation metric\n",
    "\n",
    "- precision: of examples recognized as cat, what % actually are cats?\n",
    "- recall: what % of actual cats are correctly recognized\n",
    "- F1 score: \"average\" of precision and recall\n",
    "    - $\\dfrac{2}{\\dfrac{1}{P}+\\dfrac{1}{R}}$\n",
    "\n",
    "### Satisfying and optimizaing metric\n",
    "\n",
    "- ex. maximize accuracy subject to running_time $\\le 100ms$\n",
    "- $N$ metrics: $1$ optimizing, $N-1$ satisfying\n",
    "\n",
    "### Train/dev/test distributions\n",
    "\n",
    "- choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on\n",
    "- dev and test set must come from the same distribution\n",
    "\n",
    "### Size of dev/test sets\n",
    "\n",
    "- set your test set to be big enough to give high confidence in the overall performance of your system\n",
    "\n",
    "### When to change dev/test sets and metrics\n",
    "\n",
    "- if doing well on your metric and dev/test set does not correcpond to doing well on your application, change your metric and/or dev/test set\n",
    "\n",
    "## Comparing to human-level performance\n",
    "\n",
    "### Why human-level performance\n",
    "\n",
    "- while ML is worse than human, you can\n",
    "    - get labeled data from human\n",
    "    - gain insight from manual error analysis (why did a person get this right?)\n",
    "    - better analysis of bias/variance\n",
    "    \n",
    "### Avoidable bias\n",
    "\n",
    "- human error as a proxy for bays error\n",
    "- gap between human and training error: avoidable bias\n",
    "- gap between training and dev error: variance\n",
    "\n",
    "### Two fundamental assumptions of supervised learning\n",
    "\n",
    "- you can fit the training set pretty well ~ avoidable bias\n",
    "- training set performance generalizes pretty well to dev/test set ~ variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
